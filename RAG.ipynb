{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install langchain-openai\n",
    "# !pip install langchain_pinecone\n",
    "# !pip install langchain[docarray]\n",
    "# !pip install docarray\n",
    "# !pip install pydantic==1.10.8\n",
    "# !pip install pytube \n",
    "# !pip install python-dotenv\n",
    "# !pip install tiktoken \n",
    "# !pip install pinecone-client \n",
    "# !pip install scikit-learn\n",
    "# !pip install ruff\n",
    "#!pip install langchain-ollama\n",
    "#!pip install langchain langchain-core -U\n",
    "# !pip install -U openai-whisper\n",
    "# !pip install --upgrade pytube\n",
    "# !pip install yt-dlp\n",
    "# !pip install langchain-community\n",
    "#!pip install --upgrade  langchain-community \"docarray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load enviroment variables and use any video link to load trascripts or captions.\n",
    "This caption or transcript act as source for building out Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=oVtlp72f9NQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models from Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN AI - gpt 3.5 model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "openai_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Retrieval Augmented Generation (RAG) is a machine learning model that combines the benefits of retrieval-based and generation-based models. While RAG can improve the quality of generated text by incorporating information from external knowledge sources, it is not specifically designed to address hallucination problems.\\n\\nHallucination in the context of natural language processing refers to the generation of text that is not grounded in reality or is not supported by the input data. While RAG can improve the coherence and relevance of generated text by incorporating external knowledge during the generation process, it may still produce hallucinations if the external knowledge is inaccurate or misleading.\\n\\nTherefore, while RAG may help reduce hallucination to some extent by improving the quality of generated text, it is not a guaranteed solution to hallucination problems. Researchers are actively working on developing new techniques and models to address hallucination in natural language generation, and RAG may be a part of the solution, but it is not a complete solution on its own.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 196, 'prompt_tokens': 18, 'total_tokens': 214, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-511995b8-f9f2-413a-a27c-59a05c63f83c-0', usage_metadata={'input_tokens': 18, 'output_tokens': 196, 'total_tokens': 214, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.invoke(\"Can Retrieval Augmented Generation solve hallucination problems?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama model - llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before running this we might need to run Ollama model (llama 3) in local, whereas OpenAI has API for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llama_model = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.8,\n",
    "    num_predict=256\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model.invoke(\"Can RAG solve hallucination problems?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parsing only to get the string from AI Message object returned as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation (RAG) is a natural language processing model that combines the benefits of retrieval-based and generation-based approaches to improve text generation tasks. While RAG can help improve the quality and coherence of generated text, it is unlikely to directly solve hallucination problems.\\n\\nHallucination in language generation refers to the generation of text that is not grounded in the input context or is factually incorrect. While RAG can improve the relevance of generated text by retrieving relevant information from a knowledge base, it may still struggle with generating text that is accurate and coherent.\\n\\nTo address hallucination problems in text generation, additional techniques such as fact checking, knowledge verification, and context-aware generation strategies may be necessary. While RAG can be a valuable tool in improving text generation, it is not a complete solution to hallucination problems.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = openai_model | parser # this chains the output of model into the parser. Not the string retured by the parser is stored in chain\n",
    "#equivalent:\n",
    "# output = openai_model.run(input)  # Generates output from the model\n",
    "# parsed_output = parser.parse(output)  # Processes output through the parser\n",
    "chain.invoke(\"Can Retrieval Augmented Generation solve hallucination problems?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Lewis Hamilton is eight time formula 1 world champion\\n\\nQuestion: How many times has Lewis won Formula 1 championship?\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Lewis Hamilton is eight time formula 1 world champion\", question=\"How many times has Lewis won Formula 1 championship?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the prompt is expecting two paramters: contect and question. Now chain the prompt with two parameter, model and output into the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lewis Hamilton has won the Formula 1 championship eight times.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of the prompt becomes input to the model, output of model becomes input to the parser, input of parser is stored in chain\n",
    "chain = prompt | openai_model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Lewis Hamilton is eight time formula 1 world champion\",\n",
    "    \"question\": \"How many times has Lewis won Formula 1 championship?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining another prompt action (translation) with existing prompt chain: \n",
    "\n",
    "(context, question)->model->parser->(parser_output, language)->model->parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new prompt chain\n",
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")\n",
    "# here answer will be the output of the previous chain and language will be the language to translate to given as input to the new chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'லூயிஸ் ஹேமில்டன் ஃபார்முலா 1 சாதனை எட்டு முறைகளில் வெற்றி பெற்றுள்ளார்.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | openai_model | parser\n",
    ")\n",
    "#itemgetter will get the value of the key \"language\" from the translation_prompt input\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Lewis is a formula 1 driver. Lewis Hamilton is eight time formula 1 world champion\",\n",
    "        \"question\": \"How many times has Lewis won Formula 1 championship?\",\n",
    "        \"language\": \"Tamil\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribing the captions from a test youtube video using OpenAI whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import whisper\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=u47GtXwePms\"\n",
    "\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    # Download audio using yt-dlp\n",
    "    subprocess.run([\"yt-dlp\", \"-x\", \"--audio-format\", \"mp3\", \"-o\", \"downloaded_audio.%(ext)s\", YOUTUBE_VIDEO])\n",
    "\n",
    "    # Transcribe with Whisper\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "    transcription = whisper_model.transcribe(\"downloaded_audio.mp3\", fp16=False)[\"text\"].strip()\n",
    "\n",
    "    with open(\"transcription.txt\", \"w\") as f:\n",
    "        f.write(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in entire transcript as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 47047 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Is RAG a good idea?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the transcription is going to be large, then the direct invocation might fail (max token to a model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the transcript into text loader so that we can split the whole text into various documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "type(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use text splitter to split the transcript which is loaded in to text loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"intelligences are kind of like the next stage of development. And I don't know where it leads to.\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)  # there is a overlap of 20 characters between the splits\n",
    "text_splitter.split_documents(text_documents)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have separate chunks of documents:\n",
    "\n",
    "    Challenge: We need to map which document to use or send it to the model based on the question the user gives as input.\n",
    "    \n",
    "    Solution: Store embeddings of all documents and embedding for question. Embedding in a vector space can be used to do similarity match. Use this similarity result to fihure out which document contains relavant information for the given question. (Vector embedding similarity search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings for question/prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[-0.005083421710878611, 0.0008213773253373802, -0.022214777767658234, -0.01773599162697792, -0.013884236104786396, 0.00518899317830801, -0.025490690022706985, 0.01019243709743023, -0.02641203999519348, -0.019796233624219894]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Who is Lewis Hamilton?\")\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cosine similarity of embeddings of two similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"Lewis is a formula 1 driver.\")\n",
    "sentence2 = embeddings.embed_query(\"Lewis Hamilton is eight time formula 1 world champion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8923234163986097, 0.8839253717303879)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given question(Who is Lewis Hamilton?), the similarity search of the two sentences using their embeddings are good.\n",
    "Closer to 1 indicated good match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Vector DB:\n",
    "Use embedding model get embedding and store these embeddings into vector DB. From vector DB we can do similarity match between the question and the transcript data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jjaya\\miniconda3\\envs\\dsml\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjaya\\miniconda3\\envs\\dsml\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Synthetic intelligence is described as the next stage of development in artificial intelligence, where synthetic AIs uncover and solve puzzles in the universe.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | openai_model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Setup pinecone DB - vector DB to store embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"transcript-embedding\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test similarity search of using embeddings stored and retrieved from pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='49d62313-64d3-41e3-90c8-595461da42e7', metadata={'source': 'transcription.txt'}, page_content=\"It's like high quality audio and you're speaking usually pretty clearly. I don't know what open AI's plans are either. Yeah, there's always fun projects basically. And stable diffusion also is opening up a huge amount of experimentation. I would say in the visual realm and generating images and videos and movies. I'll think like videos now. And so that's going to be pretty crazy. That's going to almost certainly work and it's going to be really interesting when the cost of content creation is going to fall to zero. You used to need a painter for a few months to paint a thing and now it's going to be speak to your phone to get your video. So Hollywood will start using it to generate scenes, which completely opens up. Yeah, so you can make a movie like Avatar eventually for under a million dollars. Much less. Maybe just by talking to your phone. I mean, I know it sounds kind of crazy. And then there'd be some voting mechanism. Like how do you have a, like, would there be a show on\"),\n",
       " Document(id='36040fd2-5376-4e7a-a89b-c064e7dfb6aa', metadata={'source': 'transcription.txt'}, page_content=\"get to another celebrity and might get into other big accounts. And then it'll just, so with just that simple goal, get them to respond. Yeah. Maximize the probability of actual response. Yeah, I mean, you could prompt a powerful model like this with their, it's opinion about how to do any possible thing you're interested in. So they will check us. They're kind of on track to become these oracles. I could sort of think of it that way. They are oracles currently is just text, but they will have calculators, they will have access to Google search, they will have all kinds of gadgets and gizmos, they will be able to operate the internet and find different information. And yeah, in some sense, that's kind of like currently what it looks like in terms of the development. Do you think it'll be an improvement eventually over what Google is for access to human knowledge? Like it'll be a more effective search engine to access human knowledge. I think there's definitely scope in building a\"),\n",
       " Document(id='dcf2011e-7bde-46d7-ab30-8ad1340cd8ec', metadata={'source': 'transcription.txt'}, page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What is Hollywood going to start doing?\")[:3]\n",
    "# It returns top 3 similar embeddings to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hollywood is going to start using AI to generate scenes, which will open up new possibilities for content creation.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | openai_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is Hollywood going to start doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    To summarize the overall flow:\n",
    "    1.  Embeddings for question is generated.\n",
    "    2.  Based on this embeddings the most similar content embedding is retrieved from vector DB (Pinecone)\n",
    "    3.  Not this content and question is sent into the LLM model\n",
    "    4.  parser gets the output and prints the string based on the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
